{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "class DCASE(Dataset):\n",
    "    def __init__(self, root_dir: str, clip_duration: int):\n",
    "        self._root_dir = Path(root_dir)\n",
    "        self._labels = pd.read_csv((self._root_dir / 'labels.csv'), names=['file', 'label'])\n",
    "        self.label_dict()\n",
    "        self._clip_duration = clip_duration\n",
    "        self._total_duration = 30 #DCASE audio length is 30s\n",
    "        self._num_clips = self._total_duration // clip_duration \n",
    "        self._data_len = len(self._labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #reading spectrograms\n",
    "        filename, label = self._labels.iloc[index]\n",
    "        filepath = self._root_dir / 'audio'/ filename\n",
    "        spec = torch.from_numpy(np.load(filepath))\n",
    "\n",
    "        #splitting spec\n",
    "        spec = self.__trim__(spec)\n",
    "        return spec, label\n",
    "\n",
    "    def __trim__(self, spec: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Trims spectrogram into multiple clips of length specified in self._num_clips\n",
    "        :param spec: tensor containing spectrogram of full audio signal of shape [1, 60, 1501]\n",
    "        :return: tensor containing stacked spectrograms of shape [num_clips, 60, clip_length] ([10, 60, 150] with 3s clips)\n",
    "        \"\"\"\n",
    "        time_steps = spec.size(-1)\n",
    "        self._num_clips = self._total_duration // self._clip_duration\n",
    "        time_interval = int(time_steps // self._num_clips)\n",
    "        all_clips = []\n",
    "        for clip_idx in range(self._num_clips):\n",
    "            start = clip_idx * time_interval\n",
    "            end = start + time_interval\n",
    "            spec_clip = spec[:, start:end]\n",
    "            #spec_clip = torch.squeeze(spec_clip)\n",
    "            all_clips.append(spec_clip)\n",
    "\n",
    "        specs = torch.stack(all_clips)\n",
    "        return specs\n",
    "    \n",
    "    def label_dict(self):\n",
    "        self._labels['label_cat'] = self._labels.label.astype('category').cat.codes.astype('int')\n",
    "        self._labels['clip_no'] = self._labels.file.apply(lambda file: (file[0], int(file[1:4])))\n",
    "        self._labels['start_frame'] = self._labels.file.apply(lambda file: file.split('_')[1])\n",
    "        self._labels['stop_frame'] = self._labels.file.apply(lambda file: file[:-4].split('_')[2])\n",
    "\n",
    "    def get_num_clips(self) -> int:\n",
    "        \"\"\"\n",
    "        Gets number of clips the raw audio has been split into\n",
    "        :return: self._num_clips of type int\n",
    "        \"\"\"\n",
    "        return self._num_clips\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._data_len\n",
    "\n",
    "    \n",
    "class NF_TRAIN_DCASE(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir: str, clip_duration: int, clip_filter):\n",
    "        self._root_dir = Path(root_dir)\n",
    "        self._labels = pd.read_csv((self._root_dir / 'labels.csv'), names=['file', 'label'])\n",
    "        self.label_dict()\n",
    "        self._labels = self._labels[self._labels['file'].isin(clip_filter)]\n",
    "        self._clip_duration = clip_duration\n",
    "        self._total_duration = 30 #DCASE audio length is 30s\n",
    "        self._num_clips = self._total_duration // clip_duration \n",
    "        self._data_len = len(self._labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #reading spectrograms\n",
    "        filename, label = self._labels.iloc[index]\n",
    "        filepath = self._root_dir / 'audio'/ filename\n",
    "        spec = torch.from_numpy(np.load(filepath))\n",
    "\n",
    "        #splitting spec\n",
    "        spec = self.__trim__(spec)\n",
    "        return spec, label\n",
    "\n",
    "    def __trim__(self, spec: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Trims spectrogram into multiple clips of length specified in self._num_clips\n",
    "        :param spec: tensor containing spectrogram of full audio signal of shape [1, 60, 1501]\n",
    "        :return: tensor containing stacked spectrograms of shape [num_clips, 60, clip_length] ([10, 60, 150] with 3s clips)\n",
    "        \"\"\"\n",
    "        time_steps = spec.size(-1)\n",
    "        self._num_clips = self._total_duration // self._clip_duration\n",
    "        time_interval = int(time_steps // self._num_clips)\n",
    "        all_clips = []\n",
    "        for clip_idx in range(self._num_clips):\n",
    "            start = clip_idx * time_interval\n",
    "            end = start + time_interval\n",
    "            spec_clip = spec[:, start:end]\n",
    "            #spec_clip = torch.squeeze(spec_clip)\n",
    "            all_clips.append(spec_clip)\n",
    "\n",
    "        specs = torch.stack(all_clips)\n",
    "        return specs\n",
    "    \n",
    "    def label_dict(self):\n",
    "        self._labels['label_cat'] = self._labels.label.astype('category').cat.codes.astype('int')\n",
    "        self._labels['clip_no'] = self._labels.file.apply(lambda file: (file[0], int(file[1:4])))\n",
    "        self._labels['start_frame'] = self._labels.file.apply(lambda file: file.split('_')[1])\n",
    "        self._labels['stop_frame'] = self._labels.file.apply(lambda file: file[:-4].split('_')[2])\n",
    "\n",
    "    def get_num_clips(self) -> int:\n",
    "        \"\"\"\n",
    "        Gets number of clips the raw audio has been split into\n",
    "        :return: self._num_clips of type int\n",
    "        \"\"\"\n",
    "        return self._num_clips\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._data_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path('./ADL_DCASE_DATA/development/')\n",
    "\n",
    "dataset = DCASE(root_dir, clip_duration=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = iter(DataLoader(dataset, shuffle=False, batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = loader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 60, 150])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>label_cat</th>\n",
       "      <th>clip_no</th>\n",
       "      <th>start_frame</th>\n",
       "      <th>stop_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001_0_30.npy</td>\n",
       "      <td>residential_area</td>\n",
       "      <td>12</td>\n",
       "      <td>(a, 1)</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a001_120_150.npy</td>\n",
       "      <td>residential_area</td>\n",
       "      <td>12</td>\n",
       "      <td>(a, 1)</td>\n",
       "      <td>120</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a001_150_180.npy</td>\n",
       "      <td>residential_area</td>\n",
       "      <td>12</td>\n",
       "      <td>(a, 1)</td>\n",
       "      <td>150</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a001_30_60.npy</td>\n",
       "      <td>residential_area</td>\n",
       "      <td>12</td>\n",
       "      <td>(a, 1)</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a001_60_90.npy</td>\n",
       "      <td>residential_area</td>\n",
       "      <td>12</td>\n",
       "      <td>(a, 1)</td>\n",
       "      <td>60</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a001_90_120.npy</td>\n",
       "      <td>residential_area</td>\n",
       "      <td>12</td>\n",
       "      <td>(a, 1)</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file             label  label_cat clip_no start_frame  \\\n",
       "0     a001_0_30.npy  residential_area         12  (a, 1)           0   \n",
       "1  a001_120_150.npy  residential_area         12  (a, 1)         120   \n",
       "2  a001_150_180.npy  residential_area         12  (a, 1)         150   \n",
       "3    a001_30_60.npy  residential_area         12  (a, 1)          30   \n",
       "4    a001_60_90.npy  residential_area         12  (a, 1)          60   \n",
       "5   a001_90_120.npy  residential_area         12  (a, 1)          90   \n",
       "\n",
       "  stop_frame  \n",
       "0         30  \n",
       "1        150  \n",
       "2        180  \n",
       "3         60  \n",
       "4         90  \n",
       "5        120  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._labels[dataset._labels['clip_no'] == ('a',1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for label, clips in labels.items():\n",
    "    total = list(range(len(clips)))\n",
    "    np.random.shuffle(total)\n",
    "    train_idx, test_idx = total[3:], total[:3]\n",
    "    \n",
    "    \n",
    "    for i in train_idx:\n",
    "        train.append(clips[i])\n",
    "    for j in test_idx:\n",
    "        test.append(clips[j])\n",
    "#     print(len(clips\n",
    "\n",
    "train_clips = []\n",
    "test_clips = []\n",
    "\n",
    "for tr in train:\n",
    "    xd = dataset._labels[dataset._labels['clip_no'] == tr].file\n",
    "    train_clips.extend(xd)\n",
    "    \n",
    "for te in test:\n",
    "    pff = dataset._labels[dataset._labels['clip_no'] == te].file\n",
    "    test_clips.extend(pff)\n",
    "    \n",
    "# dataset._labels[dataset._labels['file'].isin(train_clips)]\n",
    "\n",
    "alie = NF_DCASE(root_dir, 3, train_clips)\n",
    "fam = NF_DCASE(root_dir, 3, test_clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1170"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alie) + len(fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_TRAIN_DCASE(root_dir, 3, test_clips)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
