{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import DCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.getcwd() + '/ADL_DCASE_DATA/development'\n",
    "sequence_length = 3\n",
    "dataset = DCASE(root_dir, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(loader)\n",
    "\n",
    "data, label = iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network\n",
    "\n",
    "    Args:\n",
    "        clip_length:\n",
    "            length of the clips that the spectogram is split into\n",
    "        num_clips:\n",
    "            number of clips the spectogram is split into\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        clip_length: int, \n",
    "        num_clips: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.clip_length = clip_length\n",
    "        self.num_clips = num_clips\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=128,\n",
    "            kernel_size=5\n",
    "        )\n",
    "        self.batch1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=256,\n",
    "            kernel_size=5\n",
    "        )\n",
    "        self.batch2 = nn.BatchNorm2d(256)\n",
    "        self.pool1 = nn.MaxPool2d(\n",
    "            kernel_size=5,\n",
    "            stride=5\n",
    "        )\n",
    "        self.pool2 = nn.AdaptiveMaxPool2d((4, None))\n",
    "        self.fc1 = nn.Linear(256*4*25*num_clips, 15)\n",
    "        \n",
    "        self.initialise_layer(self.conv1)\n",
    "        self.initialise_layer(self.conv2)\n",
    "        \n",
    "    def forward(self, xs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input: [B, num_clips, H, W]\n",
    "\n",
    "        Pre-processing, change dim from:\n",
    "            [B, C, H, W] -> [C, B, H, W]\n",
    "        \n",
    "        where (C) is the number of segments the clip is split into,\n",
    "        to simulate batch of number of clip segments and uniary depth\n",
    "        \"\"\"\n",
    "        xs = xs.permute(1,0,2,3)\n",
    "        xs = self.batch1(self.conv1(xs))\n",
    "        xs = self.pool1(F.relu(xs))\n",
    "        xs = self.batch2(self.conv2(xs))\n",
    "        xs = self.pool2(F.relu(xs))\n",
    "        \"\"\"\n",
    "        Re-shape and flatten back to batch dim of 1:\n",
    "            [num_clips, X, Y, Z] -> [B, num_clips * X * Y * Z]\n",
    "            \n",
    "        where (B) = 1 so you are not left with predicitions for each clip segment,\n",
    "        just one prediction for the whole clip\n",
    "        \"\"\"\n",
    "        xs = xs.view(1,-1)\n",
    "        xs = self.fc1(xs)\n",
    "        return xs\n",
    "        \n",
    "    @staticmethod\n",
    "    def initialise_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([8]), tensor([3]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data.shape, label, np.prod(list(data.shape))\n",
    "\n",
    "model = CNN(sequence_length, dataset.get_num_clips())\n",
    "print(dataset.get_num_clips())\n",
    "out = model.forward(data)\n",
    "\n",
    "out.argmax(1), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=128,\n",
    "    kernel_size=5\n",
    ")\n",
    "batch1 = nn.BatchNorm2d(128)\n",
    "conv2 = nn.Conv2d(\n",
    "    in_channels=128,\n",
    "    out_channels=256,\n",
    "    kernel_size=5\n",
    ")\n",
    "batch2 = nn.BatchNorm2d(256)\n",
    "pool1 = nn.MaxPool2d(\n",
    "    kernel_size=5,\n",
    "    stride=5\n",
    ")\n",
    "pool2 = nn.AdaptiveMaxPool2d((4, None))\n",
    "fc1 = nn.Linear(256*4*25*dataset.get_num_clips(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.permute(1,0,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 128, 56, 146]), torch.Size([10, 1, 60, 150]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = conv1(x)\n",
    "x1.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 128, 56, 146]), torch.Size([10, 128, 56, 146]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = batch1(x1)\n",
    "x2.shape, x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 128, 11, 29]), torch.Size([10, 128, 56, 146]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = pool1(F.relu(x2))\n",
    "x3.shape, x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 256, 7, 25]), torch.Size([10, 128, 11, 29]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4 = conv2(x3)\n",
    "x4.shape, x3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 256, 7, 25]), torch.Size([10, 256, 7, 25]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x5 = batch2(x4)\n",
    "x5.shape, x4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 256, 4, 25]), torch.Size([10, 256, 7, 25]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x6 = pool2(x5)\n",
    "x6.shape, x5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256000]), torch.Size([10, 256, 4, 25]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x7 = x6.view(1, -1)\n",
    "x7.shape, x6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 15]), torch.Size([1, 256000]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x8 = fc1(x7)\n",
    "x8.shape, x7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.7419, -0.4693, -0.4379,  0.8357,  0.3683, -0.5924,  0.6473,  0.2824,\n",
       "          -0.0940, -0.2899,  0.2667,  1.0473,  0.0095, -0.7231, -0.5727]],\n",
       "        grad_fn=<AddmmBackward>), tensor([11]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x8, x8.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
